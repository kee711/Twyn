import os
import re
import json
import requests
from pathlib import Path
from functools import lru_cache
from typing import Any, Dict, List, Optional
try:
    from typing import Annotated
    from typing_extensions import TypedDict
except ImportError:
    from typing_extensions import Annotated, TypedDict
from datetime import datetime
from urllib.parse import urlparse, parse_qs

from dotenv import load_dotenv
from langgraph.graph import END, StateGraph

try:
    from tavily import TavilyClient
    import openai
    from textblob import TextBlob
    import nltk
    from sklearn.feature_extraction.text import TfidfVectorizer
    from sklearn.metrics.pairwise import cosine_similarity
    import numpy as np
except ImportError as exc:
    raise ImportError(
        "Required packages missing. Please install: tavily-python, openai, textblob, nltk, scikit-learn, numpy"
    ) from exc

load_dotenv(dotenv_path=Path(__file__).resolve().parent / ".env", override=False)

# NLTK ë°ì´í„° ë‹¤ìš´ë¡œë“œ (ì´ˆê¸° ì„¤ì •)
try:
    nltk.data.find('tokenizers/punkt')
except LookupError:
    nltk.download('punkt')

try:
    nltk.data.find('corpora/stopwords')
except LookupError:
    nltk.download('stopwords')

# íƒ€ì… ì •ì˜
class MainKeyword(TypedDict):
    keyword: str
    search_volume: int
    competition_level: str
    cpc_range: Dict[str, float]
    trend_score: float
    relevance_score: float
    regional_data: Dict[str, Any]

class SubKeywordEvaluation(TypedDict):
    keyword: str
    topic_coherence_score: float
    engagement_potential: float
    trend_momentum: float
    competition_advantage: float
    commercial_value: float
    final_score: float
    selection_reason: str

class ContentMetadata(TypedDict):
    content_id: str
    platform: str
    author_info: Dict[str, Any]
    content_type: str
    quality_score: float
    engagement_score: float
    relevance_score: float
    sentiment_score: float
    trend_momentum: float
    content_length: int
    hashtags: List[str]
    mentions: List[str]
    media_type: Optional[str]
    timestamp: datetime
    geographic_data: Optional[Dict[str, Any]]

# ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ë“¤ (ë¨¼ì € ì •ì˜)
def _merge_search_results(
    existing: Optional[Dict[str, List[Dict[str, Any]]]],
    new: Optional[Dict[str, List[Dict[str, Any]]]],
) -> Dict[str, List[Dict[str, Any]]]:
    merged: Dict[str, List[Dict[str, Any]]] = {
        platform: list(items) for platform, items in (existing or {}).items()
    }
    if not new:
        return merged

    for platform, items in new.items():
        merged.setdefault(platform, [])
        if items:
            merged[platform].extend(items)
    return merged

def _append_items(existing: Optional[List[Any]], new: Optional[List[Any]]) -> List[Any]:
    combined: List[Any] = list(existing or [])
    if new:
        combined.extend(new)
    return combined

# ê¸°ì¡´ ResearchState (í˜¸í™˜ì„± ìœ ì§€)
class ResearchState(TypedDict, total=False):
    """State tracked by the research workflow."""
    topic: str
    keywords: List[str]
    search_queries: Dict[str, str]
    search_results: Annotated[Dict[str, List[Dict[str, Any]]], _merge_search_results]
    summary: str
    references: List[Dict[str, str]]
    errors: Annotated[List[str], _append_items]

# ê³ ë„í™”ëœ ResearchState
class EnhancedResearchState(TypedDict, total=False):
    # ê¸°ì¡´ í•„ë“œ (í˜¸í™˜ì„± ìœ ì§€)
    topic: str
    keywords: List[str]
    search_queries: Dict[str, str]
    search_results: Annotated[Dict[str, List[Dict[str, Any]]], _merge_search_results]
    summary: str
    references: List[Dict[str, str]]
    errors: Annotated[List[str], _append_items]
    
    # ìƒˆë¡œìš´ í‚¤ì›Œë“œ ì¸í…”ë¦¬ì „ìŠ¤ í•„ë“œ
    main_keyword: MainKeyword
    keyword_breakdown: List[Dict[str, Any]]
    selected_sub_keywords: List[SubKeywordEvaluation]
    keyword_strategy: Dict[str, Any]
    
    # ê³ ë„í™”ëœ ê²€ìƒ‰ ê²°ê³¼ í•„ë“œ
    filtered_results: Dict[str, List[Dict[str, Any]]]
    content_quality_scores: Dict[str, float]
    engagement_metrics: Dict[str, Any]
    trend_analysis: Dict[str, Any]
    
    # ë¶„ì„ ë° ì¸ì‚¬ì´íŠ¸ í•„ë“œ
    sentiment_analysis: Dict[str, Any]
    competitive_analysis: Dict[str, Any]
    actionable_insights: List[str]
    content_recommendations: List[Dict[str, Any]]

def _normalize_topic(state) -> str:
    """ì£¼ì œ ì •ê·œí™” (ê¸°ì¡´ ë° ê³ ë„í™” ë²„ì „ ëª¨ë‘ ì§€ì›)"""
    topic = state.get("topic") or state.get("input")
    if not topic or not topic.strip():
        raise ValueError("A non-empty `topic` or `input` value is required to start the workflow.")
    return topic.strip()

def _split_keywords(topic: str) -> List[str]:
    """ê¸°ì¡´ í‚¤ì›Œë“œ ë¶„í•  í•¨ìˆ˜"""
    if "," in topic:
        keywords = [kw.strip() for kw in topic.split(",") if kw.strip()]
    else:
        keywords = [kw.strip() for kw in topic.split() if kw.strip()]
    return keywords or [topic]

def _run_tavily_query(
    query: str,
    *,
    max_results: int = 5,
    include_domains: Optional[List[str]] = None,
) -> List[Dict[str, Any]]:
    """Tavily ê²€ìƒ‰ ì‹¤í–‰"""
    client = _get_tavily_client()
    response = client.search(
        query=query,
        max_results=max_results,
        search_depth="advanced",
        include_answer=False,
        include_raw_content=False,
        include_domains=include_domains,
    )
    return response.get("results", [])

@lru_cache
def _get_tavily_client() -> TavilyClient:
    api_key = os.getenv("TAVILY_API_KEY")
    if not api_key:
        raise RuntimeError("TAVILY_API_KEY environment variable is not set.")
    return TavilyClient(api_key=api_key)

@lru_cache
def _get_openai_client():
    api_key = os.getenv("OPENAI_API_KEY")
    if not api_key:
        raise RuntimeError("OPENAI_API_KEY environment variable is not set.")
    return openai.OpenAI(api_key=api_key)

# í‚¤ì›Œë“œ ì¸í…”ë¦¬ì „ìŠ¤ í´ë˜ìŠ¤
class KeywordIntelligence:
    def __init__(self):
        self.openai_client = _get_openai_client()
        self.vectorizer = TfidfVectorizer(max_features=1000, stop_words='english')
    
    def extract_main_keyword(self, topic: str) -> MainKeyword:
        """ë©”ì¸ í‚¤ì›Œë“œ ì¶”ì¶œ ë° ë¶„ì„"""
        try:
            # OpenAIë¥¼ ì‚¬ìš©í•œ í‚¤ì›Œë“œ ì¶”ì¶œ ë° ë¶„ì„
            prompt = f"""
            ì£¼ì œ: "{topic}"
            
            ìœ„ ì£¼ì œì—ì„œ ê°€ì¥ í•µì‹¬ì ì´ê³  ê²€ìƒ‰ëŸ‰ì´ ë§ì„ ê²ƒìœ¼ë¡œ ì˜ˆìƒë˜ëŠ” ë©”ì¸ í‚¤ì›Œë“œë¥¼ ì¶”ì¶œí•˜ê³  ë¶„ì„í•´ì£¼ì„¸ìš”.
            
            ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ ì‘ë‹µí•´ì£¼ì„¸ìš”:
            {{
                "keyword": "ì¶”ì¶œëœ ë©”ì¸ í‚¤ì›Œë“œ",
                "search_volume": ì˜ˆìƒ ì›”ê°„ ê²€ìƒ‰ëŸ‰ (ìˆ«ì),
                "competition_level": "LOW/MEDIUM/HIGH",
                "relevance_score": ì£¼ì œ ê´€ë ¨ì„± ì ìˆ˜ (0-1),
                "trend_score": íŠ¸ë Œë“œ ì ìˆ˜ (0-100),
                "analysis_reason": "ì„ íƒ ì´ìœ "
            }}
            """
            
            response = self.openai_client.chat.completions.create(
                model="gpt-4",
                messages=[{"role": "user", "content": prompt}],
                temperature=0.3
            )
            
            result = json.loads(response.choices[0].message.content)
            
            return MainKeyword(
                keyword=result["keyword"],
                search_volume=result["search_volume"],
                competition_level=result["competition_level"],
                cpc_range={"min": 0.5, "max": 2.0},  # ê¸°ë³¸ê°’
                trend_score=result["trend_score"],
                relevance_score=result["relevance_score"],
                regional_data={"korea": {"popularity": 85}}
            )
            
        except Exception as e:
            # í´ë°±: ê°„ë‹¨í•œ í‚¤ì›Œë“œ ì¶”ì¶œ
            words = topic.split()
            main_word = max(words, key=len) if words else topic
            
            return MainKeyword(
                keyword=main_word,
                search_volume=1000,
                competition_level="MEDIUM",
                cpc_range={"min": 0.5, "max": 2.0},
                trend_score=70.0,
                relevance_score=0.8,
                regional_data={"korea": {"popularity": 75}}
            )
    
    def generate_keyword_breakdown(self, main_keyword: str, topic: str) -> List[Dict[str, Any]]:
        """í‚¤ì›Œë“œ ë¸Œë ˆì´í¬ë‹¤ìš´ - ì—°ê´€ í‚¤ì›Œë“œ 10ê°œ ìƒì„±"""
        try:
            prompt = f"""
            ë©”ì¸ í‚¤ì›Œë“œ: "{main_keyword}"
            ì›ë³¸ ì£¼ì œ: "{topic}"
            
            ìœ„ ë©”ì¸ í‚¤ì›Œë“œë¥¼ ê¸°ë°˜ìœ¼ë¡œ Google ê²€ìƒ‰ì—ì„œ ì—°ê´€ ê²€ìƒ‰ì–´ë¡œ ë…¸ì¶œë  ìˆ˜ ìˆê±°ë‚˜ 
            ì—°ê´€ì„±ì´ ìˆëŠ” í•˜ìœ„ í‚¤ì›Œë“œë¥¼ ì •í™•íˆ 10ê°œ ìƒì„±í•´ì£¼ì„¸ìš”.
            
            ë‹¤ìŒ ì¡°ê±´ì„ ë§Œì¡±í•´ì•¼ í•©ë‹ˆë‹¤:
            1. ë©”ì¸ í‚¤ì›Œë“œì™€ ì˜ë¯¸ì  ì—°ê´€ì„±ì´ ìˆì–´ì•¼ í•¨
            2. ì‹¤ì œ ì‚¬ìš©ìê°€ ê²€ìƒ‰í•  ë²•í•œ ìì—°ìŠ¤ëŸ¬ìš´ í‚¤ì›Œë“œ
            3. ë¡±í…Œì¼ í‚¤ì›Œë“œ í¬í•¨ (3-5ë‹¨ì–´ ì¡°í•©)
            4. ì§ˆë¬¸í˜• í‚¤ì›Œë“œ í¬í•¨ ("ì–´ë–»ê²Œ", "ì™œ", "ë¬´ì—‡" ë“±)
            
            JSON ë°°ì—´ í˜•ì‹ìœ¼ë¡œ ì‘ë‹µí•´ì£¼ì„¸ìš”:
            [
                {{"keyword": "í‚¤ì›Œë“œ1", "type": "related/longtail/question", "relevance": 0.9}},
                ...
            ]
            """
            
            response = self.openai_client.chat.completions.create(
                model="gpt-4",
                messages=[{"role": "user", "content": prompt}],
                temperature=0.5
            )
            
            keywords = json.loads(response.choices[0].message.content)
            return keywords[:10]  # ì •í™•íˆ 10ê°œë§Œ ë°˜í™˜
            
        except Exception as e:
            # í´ë°±: ê¸°ë³¸ í‚¤ì›Œë“œ ë³€í˜• ìƒì„±
            base_variations = [
                f"{main_keyword} ë°©ë²•",
                f"{main_keyword} ê°€ì´ë“œ", 
                f"{main_keyword} íŒ",
                f"{main_keyword} ì¶”ì²œ",
                f"{main_keyword} ë¹„êµ",
                f"ìµœê³ ì˜ {main_keyword}",
                f"{main_keyword} í›„ê¸°",
                f"{main_keyword} ì¥ë‹¨ì ",
                f"{main_keyword} ì„ íƒë²•",
                f"{main_keyword} íŠ¸ë Œë“œ"
            ]
            
            return [
                {"keyword": kw, "type": "related", "relevance": 0.7}
                for kw in base_variations
            ]
    
    def evaluate_sub_keywords(self, keywords: List[Dict[str, Any]], topic: str, main_keyword: str) -> List[SubKeywordEvaluation]:
        """ì„œë¸Œ í‚¤ì›Œë“œ í‰ê°€ ë° ìµœëŒ€ 2ê°œ ì„ ë³„"""
        evaluations = []
        
        for kw_data in keywords:
            keyword = kw_data["keyword"]
            
            # ê° í‰ê°€ ê¸°ì¤€ë³„ ì ìˆ˜ ê³„ì‚°
            topic_coherence = self._calculate_topic_coherence(keyword, topic)
            engagement_potential = self._predict_engagement_potential(keyword)
            trend_momentum = self._analyze_trend_momentum(keyword)
            competition_advantage = self._assess_competition_advantage(keyword)
            commercial_value = self._evaluate_commercial_value(keyword)
            
            # ê°€ì¤‘ì¹˜ ì ìš©í•œ ìµœì¢… ì ìˆ˜ ê³„ì‚°
            final_score = (
                topic_coherence * 0.30 +
                engagement_potential * 0.25 +
                trend_momentum * 0.20 +
                competition_advantage * 0.15 +
                commercial_value * 0.10
            )
            
            evaluation = SubKeywordEvaluation(
                keyword=keyword,
                topic_coherence_score=topic_coherence,
                engagement_potential=engagement_potential,
                trend_momentum=trend_momentum,
                competition_advantage=competition_advantage,
                commercial_value=commercial_value,
                final_score=final_score,
                selection_reason=self._generate_selection_reason(keyword, final_score)
            )
            
            evaluations.append(evaluation)
        
        # ì ìˆ˜ ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬í•˜ê³  ìƒìœ„ 2ê°œ ì„ íƒ
        evaluations.sort(key=lambda x: x["final_score"], reverse=True)
        return evaluations[:2]
    
    def _calculate_topic_coherence(self, keyword: str, topic: str) -> float:
        """ì£¼ì œ ì¼ê´€ì„± ì ìˆ˜ ê³„ì‚°"""
        try:
            # í…ìŠ¤íŠ¸ ìœ ì‚¬ë„ ê³„ì‚°
            texts = [keyword, topic]
            tfidf_matrix = self.vectorizer.fit_transform(texts)
            similarity = cosine_similarity(tfidf_matrix[0:1], tfidf_matrix[1:2])[0][0]
            return min(similarity * 1.2, 1.0)  # ì•½ê°„ì˜ ë¶€ìŠ¤íŒ…
        except:
            # ë‹¨ìˆœ ë‹¨ì–´ ê²¹ì¹¨ ê¸°ë°˜ ê³„ì‚°
            keyword_words = set(keyword.lower().split())
            topic_words = set(topic.lower().split())
            overlap = len(keyword_words.intersection(topic_words))
            return min(overlap / max(len(keyword_words), len(topic_words)), 1.0)
    
    def _predict_engagement_potential(self, keyword: str) -> float:
        """engagement ì ì¬ë ¥ ì˜ˆì¸¡"""
        # í‚¤ì›Œë“œ íŠ¹ì„± ê¸°ë°˜ engagement ì˜ˆì¸¡
        engagement_indicators = [
            "ë°©ë²•", "ê°€ì´ë“œ", "íŒ", "ì¶”ì²œ", "í›„ê¸°", "ë¹„êµ", "ìˆœìœ„", "best", "top"
        ]
        
        score = 0.5  # ê¸°ë³¸ ì ìˆ˜
        for indicator in engagement_indicators:
            if indicator in keyword.lower():
                score += 0.1
        
        # ì§ˆë¬¸í˜• í‚¤ì›Œë“œëŠ” engagementê°€ ë†’ìŒ
        if any(q in keyword for q in ["ì–´ë–»ê²Œ", "ì™œ", "ë¬´ì—‡", "ì–¸ì œ", "ì–´ë””ì„œ"]):
            score += 0.2
            
        return min(score, 1.0)
    
    def _analyze_trend_momentum(self, keyword: str) -> float:
        """íŠ¸ë Œë“œ ëª¨ë©˜í…€ ë¶„ì„"""
        # íŠ¸ë Œë“œ í‚¤ì›Œë“œ íŒ¨í„´ ë¶„ì„
        trend_words = ["2024", "ìµœì‹ ", "ì‹ ê·œ", "ìƒˆë¡œìš´", "íŠ¸ë Œë“œ", "ì¸ê¸°", "í•«"]
        
        score = 0.6  # ê¸°ë³¸ ì ìˆ˜
        for word in trend_words:
            if word in keyword:
                score += 0.1
                
        return min(score, 1.0)
    
    def _assess_competition_advantage(self, keyword: str) -> float:
        """ê²½ìŸ ìš°ìœ„ë„ í‰ê°€"""
        # ë¡±í…Œì¼ í‚¤ì›Œë“œëŠ” ê²½ìŸì´ ë‚®ìŒ
        word_count = len(keyword.split())
        if word_count >= 4:
            return 0.8
        elif word_count == 3:
            return 0.6
        else:
            return 0.4
    
    def _evaluate_commercial_value(self, keyword: str) -> float:
        """ìƒì—…ì  ê°€ì¹˜ í‰ê°€"""
        commercial_indicators = [
            "êµ¬ë§¤", "ê°€ê²©", "ë¹„ìš©", "í• ì¸", "ì¶”ì²œ", "ìˆœìœ„", "ë¹„êµ", "ë¦¬ë·°", "í›„ê¸°"
        ]
        
        score = 0.3  # ê¸°ë³¸ ì ìˆ˜
        for indicator in commercial_indicators:
            if indicator in keyword:
                score += 0.15
                
        return min(score, 1.0)
    
    def _generate_selection_reason(self, keyword: str, score: float) -> str:
        """ì„ íƒ ì‚¬ìœ  ìƒì„±"""
        if score >= 0.8:
            return f"'{keyword}'ëŠ” ë†’ì€ ì£¼ì œ ê´€ë ¨ì„±ê³¼ engagement ì ì¬ë ¥ì„ ë³´ì—¬ ìµœìš°ì„  ì„ íƒë˜ì—ˆìŠµë‹ˆë‹¤."
        elif score >= 0.6:
            return f"'{keyword}'ëŠ” ê· í˜•ì¡íŒ ì„±ê³¼ ì§€í‘œë¡œ ì „ëµì  ê°€ì¹˜ê°€ ë†’ìŠµë‹ˆë‹¤."
        else:
            return f"'{keyword}'ëŠ” ê¸°ë³¸ì ì¸ ìš”êµ¬ì‚¬í•­ì„ ì¶©ì¡±í•˜ì—¬ ì„ íƒë˜ì—ˆìŠµë‹ˆë‹¤."

# ì½˜í…ì¸  í•„í„°ë§ í´ë˜ìŠ¤
class ContentFilter:
    def __init__(self):
        self.profile_patterns = [
            r'/profile/', r'/user/', r'/u/', r'/@', r'/about', r'/bio'
        ]
        self.reply_patterns = [
            r'/status/\d+/reply', r'/thread/', r'/comment/', r'/replies'
        ]
        self.spam_keywords = [
            'follow me', 'check bio', 'link in bio', 'dm me', 'subscribe'
        ]
    
    def is_valid_content(self, content: Dict[str, Any]) -> bool:
        """ì½˜í…ì¸  ìœ íš¨ì„± ê²€ì‚¬"""
        url = content.get('url', '')
        text = content.get('content', '') or content.get('snippet', '')
        
        # URL íŒ¨í„´ìœ¼ë¡œ í”„ë¡œí•„ í˜ì´ì§€ ì œì™¸
        if self._is_profile_page(url):
            return False
        
        # í•˜ìœ„ ì“°ë ˆë“œ/ëŒ“ê¸€ ì œì™¸
        if self._is_reply_or_comment(url, content):
            return False
        
        # í…ìŠ¤íŠ¸ í’ˆì§ˆ ê²€ì‚¬
        if not self._meets_quality_threshold(text):
            return False
        
        # ìŠ¤íŒ¸ ì½˜í…ì¸  ì œì™¸
        if self._is_spam_content(text):
            return False
        
        return True
    
    def _is_profile_page(self, url: str) -> bool:
        """í”„ë¡œí•„ í˜ì´ì§€ ì—¬ë¶€ í™•ì¸"""
        return any(pattern in url.lower() for pattern in self.profile_patterns)
    
    def _is_reply_or_comment(self, url: str, content: Dict[str, Any]) -> bool:
        """ë‹µê¸€/ëŒ“ê¸€ ì—¬ë¶€ í™•ì¸"""
        # URL íŒ¨í„´ í™•ì¸
        if any(pattern in url.lower() for pattern in self.reply_patterns):
            return True
        
        # ì½˜í…ì¸  êµ¬ì¡° í™•ì¸ (ë‹µê¸€ íŠ¹ì„±)
        text = content.get('content', '') or content.get('snippet', '')
        if text.startswith('@') or text.startswith('Re:'):
            return True
        
        return False
    
    def _meets_quality_threshold(self, text: str) -> bool:
        """í…ìŠ¤íŠ¸ í’ˆì§ˆ ì„ê³„ê°’ í™•ì¸"""
        if not text or len(text.strip()) < 50:
            return False
        
        # ì˜ë¯¸ìˆëŠ” ë‹¨ì–´ ë¹„ìœ¨ í™•ì¸
        words = text.split()
        if len(words) < 10:
            return False
        
        return True
    
    def _is_spam_content(self, text: str) -> bool:
        """ìŠ¤íŒ¸ ì½˜í…ì¸  ì—¬ë¶€ í™•ì¸"""
        text_lower = text.lower()
        spam_count = sum(1 for keyword in self.spam_keywords if keyword in text_lower)
        
        # ìŠ¤íŒ¸ í‚¤ì›Œë“œê°€ 2ê°œ ì´ìƒì´ë©´ ìŠ¤íŒ¸ìœ¼ë¡œ íŒë‹¨
        return spam_count >= 2
    
    def calculate_content_quality(self, content: Dict[str, Any]) -> float:
        """ì½˜í…ì¸  í’ˆì§ˆ ì ìˆ˜ ê³„ì‚°"""
        text = content.get('content', '') or content.get('snippet', '')
        
        quality_score = 0.0
        
        # í…ìŠ¤íŠ¸ ê¸¸ì´ ì ìˆ˜ (0.3 ê°€ì¤‘ì¹˜)
        length_score = min(len(text) / 500, 1.0) * 0.3
        quality_score += length_score
        
        # ì–¸ì–´ í’ˆì§ˆ ì ìˆ˜ (0.2 ê°€ì¤‘ì¹˜)
        try:
            blob = TextBlob(text)
            # ë¬¸ì¥ ìˆ˜ì™€ ë‹¨ì–´ ìˆ˜ì˜ ë¹„ìœ¨ë¡œ ê°€ë…ì„± ì¸¡ì •
            sentences = len(blob.sentences)
            words = len(blob.words)
            readability = min(words / max(sentences, 1) / 20, 1.0) * 0.2
            quality_score += readability
        except:
            quality_score += 0.1  # ê¸°ë³¸ ì ìˆ˜
        
        # ì •ë³´ ë°€ë„ ì ìˆ˜ (0.3 ê°€ì¤‘ì¹˜)
        unique_words = len(set(text.lower().split()))
        total_words = len(text.split())
        density = (unique_words / max(total_words, 1)) * 0.3
        quality_score += density
        
        # engagement ì§€í‘œ ì ìˆ˜ (0.2 ê°€ì¤‘ì¹˜)
        engagement_indicators = ['?', '!', '#', '@']
        engagement_count = sum(text.count(indicator) for indicator in engagement_indicators)
        engagement_score = min(engagement_count / 10, 1.0) * 0.2
        quality_score += engagement_score
        
        return min(quality_score, 1.0)

# ê³ ê¸‰ ê²€ìƒ‰ ì¿¼ë¦¬ ìƒì„±ê¸°
class AdvancedSearchQuery:
    def generate_queries(self, main_keyword: str, sub_keywords: List[str]) -> Dict[str, List[str]]:
        """ê³ ë„í™”ëœ ê²€ìƒ‰ ì¿¼ë¦¬ ìƒì„±"""
        primary_queries = [
            f'"{main_keyword}" -profile -bio -about -"follow me" -"check bio"',
            f'({main_keyword}) AND (viral OR trending OR popular) -reply -comment',
        ]
        
        secondary_queries = []
        for sub_keyword in sub_keywords:
            secondary_queries.append(f'({sub_keyword}) AND ({main_keyword}) -thread/ -status/')
        
        exclusion_patterns = [
            '-"follow me"', '-"check bio"', '-"link in bio"', '-"dm me"',
            '-reply', '-comment', '-thread/', '-status/', '-profile', '-bio'
        ]
        
        quality_filters = [
            'min_length:50', 'has_engagement:true'
        ]
        
        return {
            'primary_queries': primary_queries,
            'secondary_queries': secondary_queries,
            'exclusion_patterns': exclusion_patterns,
            'quality_filters': quality_filters
        }
    
    def optimize_for_platform(self, base_query: str, platform: str) -> str:
        """í”Œë«í¼ë³„ ì¿¼ë¦¬ ìµœì í™”"""
        if platform == 'threads':
            # Threads ìµœì í™”: ì›ë³¸ ê²Œì‹œê¸€ ìš°ì„ 
            return f"site:threads.net {base_query} -/reply -/comment"
        elif platform == 'x':
            # X ìµœì í™”: ë¦¬íŠ¸ìœ— ì œì™¸, ì›ë³¸ íŠ¸ìœ— ìš°ì„ 
            return f"(site:x.com OR site:twitter.com) {base_query} -RT -retweet"
        else:
            return base_query

# ê¸°ì¡´ ê¸°ë³¸ ì›Œí¬í”Œë¡œìš° í•¨ìˆ˜ë“¤ (í˜¸í™˜ì„± ìœ ì§€)
def keyword_planner(state: ResearchState) -> ResearchState:
    """ê¸°ì¡´ í‚¤ì›Œë“œ í”Œë˜ë„ˆ"""
    topic = _normalize_topic(state)
    keywords = _split_keywords(topic)

    combined_keywords = " ".join(keywords)
    return {
        "topic": topic,
        "keywords": keywords,
        "search_queries": {
            "threads": f"site:threads.net {combined_keywords} reference",
            "x": f"({combined_keywords}) (site:x.com OR site:twitter.com) reference",
        },
    }

def search_threads(state: ResearchState) -> ResearchState:
    """ê¸°ì¡´ Threads ê²€ìƒ‰"""
    queries = state.get("search_queries", {})
    query = queries.get("threads")
    results: List[Dict[str, Any]] = []
    errors: List[str] = []
    if query:
        try:
            results = _run_tavily_query(query, include_domains=["threads.com"])
        except Exception as exc:
            errors.append(f"Threads search failed: {exc}")
    else:
        errors.append("Threads search skipped: query missing.")

    updates: ResearchState = {"search_results": {"threads": results}}
    if errors:
        updates["errors"] = errors
    return updates

def search_x(state: ResearchState) -> ResearchState:
    """ê¸°ì¡´ X ê²€ìƒ‰"""
    queries = state.get("search_queries", {})
    query = queries.get("x")
    results: List[Dict[str, Any]] = []
    errors: List[str] = []
    if query:
        try:
            results = _run_tavily_query(query, include_domains=["x.com"])
        except Exception as exc:
            errors.append(f"X search failed: {exc}")
    else:
        errors.append("X search skipped: query missing.")

    updates: ResearchState = {"search_results": {"x": results}}
    if errors:
        updates["errors"] = errors
    return updates

def _summarize_platform(platform: str, items: List[Dict[str, Any]]) -> Optional[str]:
    """í”Œë«í¼ë³„ ìš”ì•½ ìƒì„±"""
    if not items:
        return None

    highlights: List[str] = []
    for item in items[:2]:  # grab top highlights
        snippet = item.get("content") or item.get("snippet") or ""
        snippet = " ".join(snippet.split())  # collapse whitespace
        if not snippet:
            snippet = "ì–¸ê¸‰ëœ ê²Œì‹œê¸€ì— ëŒ€í•œ ì¶”ê°€ ì„¤ëª… ì—†ìŒ"
        if len(snippet) > 200:
            snippet = snippet[:197].rstrip() + "..."
        highlights.append(snippet)

    platform_label = "Threads" if platform == "threads" else "X"
    return f"{platform_label}: {' / '.join(highlights)}"

def summarize_results(state: ResearchState) -> ResearchState:
    """ê¸°ì¡´ ê²°ê³¼ ìš”ì•½"""
    results = state.get("search_results", {})
    summary_lines: List[str] = []
    references: List[Dict[str, str]] = []

    for platform in ("threads", "x"):
        items = results.get(platform, [])
        summary_line = _summarize_platform(platform, items)
        if summary_line:
            summary_lines.append(summary_line)
        else:
            summary_lines.append(
                "Threadsì—ì„œ ë°œê²¬ëœ ìë£Œê°€ ì—†ìŠµë‹ˆë‹¤." if platform == "threads" else "Xì—ì„œ ë°œê²¬ëœ ìë£Œê°€ ì—†ìŠµë‹ˆë‹¤."
            )

        for item in items:
            reference = {
                "platform": "Threads" if platform == "threads" else "X",
                "title": item.get("title") or item.get("url", ""),
                "url": item.get("url", ""),
                "snippet": (item.get("content") or item.get("snippet") or "").strip(),
            }
            references.append(reference)

    if state.get("errors"):
        summary_lines.append("ì˜¤ë¥˜: " + " | ".join(state["errors"]))

    state["summary"] = "\n".join(summary_lines)
    state["references"] = references
    return state

# ê³ ë„í™”ëœ ì›Œí¬í”Œë¡œìš° í•¨ìˆ˜ë“¤
def extract_main_keyword(state: EnhancedResearchState) -> EnhancedResearchState:
    """ë©”ì¸ í‚¤ì›Œë“œ ì¶”ì¶œ"""
    topic = _normalize_topic(state)
    
    try:
        keyword_intelligence = KeywordIntelligence()
        main_keyword = keyword_intelligence.extract_main_keyword(topic)
        
        return {
            "topic": topic,
            "main_keyword": main_keyword,
            "keyword_strategy": {
                "phase": "main_keyword_extracted",
                "confidence": main_keyword["relevance_score"]
            }
        }
    except Exception as e:
        return {
            "topic": topic,
            "errors": [f"Main keyword extraction failed: {str(e)}"]
        }

def generate_keyword_breakdown(state: EnhancedResearchState) -> EnhancedResearchState:
    """í‚¤ì›Œë“œ ë¸Œë ˆì´í¬ë‹¤ìš´"""
    main_keyword_data = state.get("main_keyword")
    if not main_keyword_data:
        return {"errors": ["Main keyword not found"]}
    
    topic = state.get("topic", "")
    main_keyword = main_keyword_data["keyword"]
    
    try:
        keyword_intelligence = KeywordIntelligence()
        breakdown = keyword_intelligence.generate_keyword_breakdown(main_keyword, topic)
        
        return {
            "keyword_breakdown": breakdown,
            "keyword_strategy": {
                **state.get("keyword_strategy", {}),
                "phase": "breakdown_completed",
                "breakdown_count": len(breakdown)
            }
        }
    except Exception as e:
        return {"errors": [f"Keyword breakdown failed: {str(e)}"]}

def evaluate_sub_keywords(state: EnhancedResearchState) -> EnhancedResearchState:
    """ì„œë¸Œ í‚¤ì›Œë“œ í‰ê°€ ë° ì„ ë³„"""
    breakdown = state.get("keyword_breakdown", [])
    if not breakdown:
        return {"errors": ["Keyword breakdown not found"]}
    
    topic = state.get("topic", "")
    main_keyword = state.get("main_keyword", {}).get("keyword", "")
    
    try:
        keyword_intelligence = KeywordIntelligence()
        selected_keywords = keyword_intelligence.evaluate_sub_keywords(breakdown, topic, main_keyword)
        
        return {
            "selected_sub_keywords": selected_keywords,
            "keyword_strategy": {
                **state.get("keyword_strategy", {}),
                "phase": "evaluation_completed",
                "selected_count": len(selected_keywords),
                "top_scores": [kw["final_score"] for kw in selected_keywords]
            }
        }
    except Exception as e:
        return {"errors": [f"Sub keyword evaluation failed: {str(e)}"]}

def generate_advanced_queries(state: EnhancedResearchState) -> EnhancedResearchState:
    """ê³ ê¸‰ ê²€ìƒ‰ ì¿¼ë¦¬ ìƒì„±"""
    main_keyword_data = state.get("main_keyword")
    selected_keywords = state.get("selected_sub_keywords", [])
    
    if not main_keyword_data:
        return {"errors": ["Main keyword not found for query generation"]}
    
    main_keyword = main_keyword_data["keyword"]
    sub_keywords = [kw["keyword"] for kw in selected_keywords]
    
    try:
        query_generator = AdvancedSearchQuery()
        query_structure = query_generator.generate_queries(main_keyword, sub_keywords)
        
        # í”Œë«í¼ë³„ ìµœì í™”ëœ ì¿¼ë¦¬ ìƒì„±
        search_queries = {}
        
        for platform in ['threads', 'x']:
            primary_query = query_structure['primary_queries'][0]
            optimized_query = query_generator.optimize_for_platform(primary_query, platform)
            search_queries[platform] = optimized_query
        
        return {
            "search_queries": search_queries,
            "keyword_strategy": {
                **state.get("keyword_strategy", {}),
                "phase": "queries_generated",
                "query_count": len(search_queries)
            }
        }
    except Exception as e:
        return {"errors": [f"Query generation failed: {str(e)}"]}

def search_threads_enhanced(state: EnhancedResearchState) -> EnhancedResearchState:
    """ê³ ë„í™”ëœ Threads ê²€ìƒ‰"""
    queries = state.get("search_queries", {})
    query = queries.get("threads")
    results: List[Dict[str, Any]] = []
    errors: List[str] = []
    
    if query:
        try:
            client = _get_tavily_client()
            response = client.search(
                query=query,
                max_results=10,  # ë” ë§ì€ ê²°ê³¼ ìˆ˜ì§‘
                search_depth="advanced",
                include_answer=False,
                include_raw_content=False,
                include_domains=["threads.net"]
            )
            raw_results = response.get("results", [])
            
            # ì½˜í…ì¸  í•„í„°ë§ ì ìš©
            content_filter = ContentFilter()
            filtered_results = []
            
            for result in raw_results:
                if content_filter.is_valid_content(result):
                    # í’ˆì§ˆ ì ìˆ˜ ê³„ì‚°
                    quality_score = content_filter.calculate_content_quality(result)
                    result['quality_score'] = quality_score
                    result['platform'] = 'threads'
                    filtered_results.append(result)
            
            # í’ˆì§ˆ ì ìˆ˜ ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬
            results = sorted(filtered_results, key=lambda x: x.get('quality_score', 0), reverse=True)[:5]
            
        except Exception as exc:
            errors.append(f"Enhanced Threads search failed: {exc}")
    else:
        errors.append("Threads search skipped: query missing.")

    updates: EnhancedResearchState = {"search_results": {"threads": results}}
    if errors:
        updates["errors"] = errors
    return updates

def search_x_enhanced(state: EnhancedResearchState) -> EnhancedResearchState:
    """ê³ ë„í™”ëœ X ê²€ìƒ‰"""
    queries = state.get("search_queries", {})
    query = queries.get("x")
    results: List[Dict[str, Any]] = []
    errors: List[str] = []
    
    if query:
        try:
            client = _get_tavily_client()
            response = client.search(
                query=query,
                max_results=10,
                search_depth="advanced",
                include_answer=False,
                include_raw_content=False,
                include_domains=["x.com", "twitter.com"]
            )
            raw_results = response.get("results", [])
            
            # ì½˜í…ì¸  í•„í„°ë§ ì ìš©
            content_filter = ContentFilter()
            filtered_results = []
            
            for result in raw_results:
                if content_filter.is_valid_content(result):
                    quality_score = content_filter.calculate_content_quality(result)
                    result['quality_score'] = quality_score
                    result['platform'] = 'x'
                    filtered_results.append(result)
            
            results = sorted(filtered_results, key=lambda x: x.get('quality_score', 0), reverse=True)[:5]
            
        except Exception as exc:
            errors.append(f"Enhanced X search failed: {exc}")
    else:
        errors.append("X search skipped: query missing.")

    updates: EnhancedResearchState = {"search_results": {"x": results}}
    if errors:
        updates["errors"] = errors
    return updates

def analyze_engagement_potential(state: EnhancedResearchState) -> EnhancedResearchState:
    """engagement ì ì¬ë ¥ ë¶„ì„"""
    search_results = state.get("search_results", {})
    
    engagement_metrics = {
        "total_content_analyzed": 0,
        "average_quality_score": 0.0,
        "platform_performance": {},
        "content_type_distribution": {},
        "sentiment_distribution": {"positive": 0, "neutral": 0, "negative": 0}
    }
    
    all_results = []
    for platform, results in search_results.items():
        all_results.extend(results)
        
        if results:
            avg_quality = sum(r.get('quality_score', 0) for r in results) / len(results)
            engagement_metrics["platform_performance"][platform] = {
                "content_count": len(results),
                "average_quality": avg_quality,
                "top_quality_score": max(r.get('quality_score', 0) for r in results)
            }
    
    engagement_metrics["total_content_analyzed"] = len(all_results)
    
    if all_results:
        engagement_metrics["average_quality_score"] = sum(
            r.get('quality_score', 0) for r in all_results
        ) / len(all_results)
        
        # ê°ì • ë¶„ì„
        for result in all_results:
            text = result.get('content', '') or result.get('snippet', '')
            try:
                blob = TextBlob(text)
                sentiment = blob.sentiment.polarity
                if sentiment > 0.1:
                    engagement_metrics["sentiment_distribution"]["positive"] += 1
                elif sentiment < -0.1:
                    engagement_metrics["sentiment_distribution"]["negative"] += 1
                else:
                    engagement_metrics["sentiment_distribution"]["neutral"] += 1
            except:
                engagement_metrics["sentiment_distribution"]["neutral"] += 1
    
    return {"engagement_metrics": engagement_metrics}

def generate_content_strategy(state: EnhancedResearchState) -> EnhancedResearchState:
    """ì½˜í…ì¸  ì „ëµ ë° ì¸ì‚¬ì´íŠ¸ ìƒì„±"""
    main_keyword = state.get("main_keyword", {})
    selected_keywords = state.get("selected_sub_keywords", [])
    engagement_metrics = state.get("engagement_metrics", {})
    search_results = state.get("search_results", {})
    
    # ì•¡ì…˜ ê°€ëŠ¥í•œ ì¸ì‚¬ì´íŠ¸ ìƒì„±
    actionable_insights = []
    content_recommendations = []
    
    # í‚¤ì›Œë“œ ì „ëµ ì¸ì‚¬ì´íŠ¸
    if main_keyword:
        keyword_name = main_keyword.get("keyword", "")
        trend_score = main_keyword.get("trend_score", 0)
        
        if trend_score > 80:
            actionable_insights.append(
                f"'{keyword_name}' í‚¤ì›Œë“œëŠ” ë†’ì€ íŠ¸ë Œë“œ ì ìˆ˜({trend_score})ë¥¼ ë³´ì…ë‹ˆë‹¤. ì¦‰ì‹œ ì½˜í…ì¸  ì œì‘ì„ ì‹œì‘í•˜ì„¸ìš”."
            )
        elif trend_score > 60:
            actionable_insights.append(
                f"'{keyword_name}' í‚¤ì›Œë“œëŠ” ì•ˆì •ì ì¸ íŠ¸ë Œë“œë¥¼ ë³´ì…ë‹ˆë‹¤. ì§€ì†ì ì¸ ì½˜í…ì¸  ì „ëµì— ì í•©í•©ë‹ˆë‹¤."
            )
    
    # ì„œë¸Œ í‚¤ì›Œë“œ ì¸ì‚¬ì´íŠ¸
    if selected_keywords:
        top_keyword = selected_keywords[0]
        actionable_insights.append(
            f"ìµœìš°ì„  ì„œë¸Œ í‚¤ì›Œë“œ '{top_keyword['keyword']}'ëŠ” {top_keyword['final_score']:.2f} ì ìˆ˜ë¡œ ì„ ë³„ë˜ì—ˆìŠµë‹ˆë‹¤. "
            f"{top_keyword['selection_reason']}"
        )
    
    # engagement ë¶„ì„ ì¸ì‚¬ì´íŠ¸
    if engagement_metrics:
        avg_quality = engagement_metrics.get("average_quality_score", 0)
        if avg_quality > 0.7:
            actionable_insights.append("ì‹œì¥ì˜ ì½˜í…ì¸  í’ˆì§ˆì´ ë†’ìŠµë‹ˆë‹¤. ì°¨ë³„í™”ëœ ê³ í’ˆì§ˆ ì½˜í…ì¸ ê°€ í•„ìš”í•©ë‹ˆë‹¤.")
        elif avg_quality < 0.5:
            actionable_insights.append("ì‹œì¥ì˜ ì½˜í…ì¸  í’ˆì§ˆì´ ë‚®ìŠµë‹ˆë‹¤. í’ˆì§ˆ ê°œì„ ìœ¼ë¡œ ê²½ìŸ ìš°ìœ„ë¥¼ í™•ë³´í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
        
        # í”Œë«í¼ë³„ ì„±ê³¼ ë¶„ì„
        platform_performance = engagement_metrics.get("platform_performance", {})
        if platform_performance:
            best_platform = max(platform_performance.items(), key=lambda x: x[1]["average_quality"])
            actionable_insights.append(
                f"{best_platform[0].title()} í”Œë«í¼ì—ì„œ ê°€ì¥ ë†’ì€ í’ˆì§ˆì˜ ì½˜í…ì¸ ê°€ ë°œê²¬ë˜ì—ˆìŠµë‹ˆë‹¤. "
                f"ì´ í”Œë«í¼ì„ ìš°ì„ ì ìœ¼ë¡œ í™œìš©í•˜ì„¸ìš”."
            )
    
    # ì½˜í…ì¸  ì¶”ì²œ ìƒì„±
    if selected_keywords:
        for keyword_data in selected_keywords:
            keyword = keyword_data["keyword"]
            engagement_score = keyword_data["engagement_potential"]
            
            if engagement_score > 0.7:
                content_type = "ìƒí˜¸ì‘ìš©í˜• ì½˜í…ì¸  (Q&A, íˆ¬í‘œ, í† ë¡ )"
            elif engagement_score > 0.5:
                content_type = "ì •ë³´ ì œê³µí˜• ì½˜í…ì¸  (ê°€ì´ë“œ, íŒ, í•˜ìš°íˆ¬)"
            else:
                content_type = "ê¸°ë³¸ ì •ë³´í˜• ì½˜í…ì¸  (ì†Œê°œ, ì„¤ëª…)"
            
            content_recommendations.append({
                "keyword": keyword,
                "recommended_content_type": content_type,
                "expected_engagement": engagement_score,
                "priority": "high" if engagement_score > 0.7 else "medium"
            })
    
    # ê²½ìŸ ë¶„ì„
    competitive_analysis = {
        "market_saturation": "medium",  # ê¸°ë³¸ê°’
        "content_gap_opportunities": [],
        "differentiation_strategies": []
    }
    
    # ì½˜í…ì¸  ê°­ ë¶„ì„
    all_content = []
    for platform_results in search_results.values():
        for result in platform_results:
            content = result.get('content', '') or result.get('snippet', '')
            all_content.append(content.lower())
    
    if all_content:
        # ìì£¼ ì–¸ê¸‰ë˜ì§€ ì•ŠëŠ” ê´€ë ¨ ì£¼ì œ ì°¾ê¸°
        common_words = set()
        for content in all_content:
            words = content.split()
            common_words.update(words)
        
        # ì ì¬ì  ì½˜í…ì¸  ê°­ ì‹ë³„
        gap_opportunities = [
            "ì‹¤ì œ ì‚¬ìš©ì ê²½í—˜ë‹´",
            "ë‹¨ê³„ë³„ ì‹¤í–‰ ê°€ì´ë“œ", 
            "ë¹„êµ ë¶„ì„ ì½˜í…ì¸ ",
            "ì „ë¬¸ê°€ ì¸í„°ë·°",
            "ë°ì´í„° ê¸°ë°˜ ì¸ì‚¬ì´íŠ¸"
        ]
        
        competitive_analysis["content_gap_opportunities"] = gap_opportunities[:3]
        competitive_analysis["differentiation_strategies"] = [
            "ë…ì°½ì ì¸ ê´€ì  ì œì‹œ",
            "ì‹¤ìš©ì ì¸ ì•¡ì…˜ ì•„ì´í…œ í¬í•¨",
            "ì‹œê°ì  ìš”ì†Œ ê°•í™”"
        ]
    
    return {
        "actionable_insights": actionable_insights,
        "content_recommendations": content_recommendations,
        "competitive_analysis": competitive_analysis,
        "keyword_strategy": {
            **state.get("keyword_strategy", {}),
            "phase": "strategy_completed",
            "insights_count": len(actionable_insights),
            "recommendations_count": len(content_recommendations)
        }
    }

def summarize_enhanced_results(state: EnhancedResearchState) -> EnhancedResearchState:
    """ê³ ë„í™”ëœ ê²°ê³¼ ìš”ì•½"""
    main_keyword = state.get("main_keyword", {})
    selected_keywords = state.get("selected_sub_keywords", [])
    search_results = state.get("search_results", {})
    actionable_insights = state.get("actionable_insights", [])
    engagement_metrics = state.get("engagement_metrics", {})
    
    summary_lines = []
    references = []
    
    # í‚¤ì›Œë“œ ì „ëµ ìš”ì•½
    if main_keyword:
        keyword_name = main_keyword.get("keyword", "")
        search_volume = main_keyword.get("search_volume", 0)
        trend_score = main_keyword.get("trend_score", 0)
        
        summary_lines.append(f"ğŸ¯ ë©”ì¸ í‚¤ì›Œë“œ: '{keyword_name}' (ì˜ˆìƒ ê²€ìƒ‰ëŸ‰: {search_volume:,}, íŠ¸ë Œë“œ ì ìˆ˜: {trend_score})")
    
    if selected_keywords:
        summary_lines.append("ğŸ” ì„ ë³„ëœ ì„œë¸Œ í‚¤ì›Œë“œ:")
        for i, kw in enumerate(selected_keywords, 1):
            summary_lines.append(f"  {i}. '{kw['keyword']}' (ì ìˆ˜: {kw['final_score']:.2f})")
    
    # í”Œë«í¼ë³„ ê²€ìƒ‰ ê²°ê³¼ ìš”ì•½
    summary_lines.append("\nğŸ“Š í”Œë«í¼ë³„ ë¶„ì„ ê²°ê³¼:")
    
    for platform in ("threads", "x"):
        items = search_results.get(platform, [])
        platform_label = "Threads" if platform == "threads" else "X"
        
        if items:
            avg_quality = sum(item.get('quality_score', 0) for item in items) / len(items)
            summary_lines.append(f"  {platform_label}: {len(items)}ê°œ ê³ í’ˆì§ˆ ì½˜í…ì¸  ë°œê²¬ (í‰ê·  í’ˆì§ˆ: {avg_quality:.2f})")
            
            # ìƒìœ„ 2ê°œ í•˜ì´ë¼ì´íŠ¸
            for item in items[:2]:
                snippet = item.get("content") or item.get("snippet") or ""
                snippet = " ".join(snippet.split())
                if len(snippet) > 150:
                    snippet = snippet[:147] + "..."
                summary_lines.append(f"    â€¢ {snippet}")
        else:
            summary_lines.append(f"  {platform_label}: ê´€ë ¨ ì½˜í…ì¸ ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
        
        # ì°¸ì¡° ë§í¬ ìˆ˜ì§‘
        for item in items:
            reference = {
                "platform": platform_label,
                "title": item.get("title") or item.get("url", ""),
                "url": item.get("url", ""),
                "snippet": (item.get("content") or item.get("snippet") or "").strip(),
                "quality_score": item.get("quality_score", 0)
            }
            references.append(reference)
    
    # engagement ë©”íŠ¸ë¦­ ìš”ì•½
    if engagement_metrics:
        total_analyzed = engagement_metrics.get("total_content_analyzed", 0)
        avg_quality = engagement_metrics.get("average_quality_score", 0)
        summary_lines.append(f"\nğŸ“ˆ ë¶„ì„ ê²°ê³¼: ì´ {total_analyzed}ê°œ ì½˜í…ì¸  ë¶„ì„, í‰ê·  í’ˆì§ˆ ì ìˆ˜ {avg_quality:.2f}")
        
        sentiment_dist = engagement_metrics.get("sentiment_distribution", {})
        if sentiment_dist:
            positive = sentiment_dist.get("positive", 0)
            neutral = sentiment_dist.get("neutral", 0)
            negative = sentiment_dist.get("negative", 0)
            summary_lines.append(f"  ê°ì • ë¶„ì„: ê¸ì • {positive}ê°œ, ì¤‘ë¦½ {neutral}ê°œ, ë¶€ì • {negative}ê°œ")
    
    # í•µì‹¬ ì¸ì‚¬ì´íŠ¸ ìš”ì•½
    if actionable_insights:
        summary_lines.append("\nğŸ’¡ í•µì‹¬ ì¸ì‚¬ì´íŠ¸:")
        for insight in actionable_insights[:3]:  # ìƒìœ„ 3ê°œë§Œ í‘œì‹œ
            summary_lines.append(f"  â€¢ {insight}")
    
    # ì—ëŸ¬ ë©”ì‹œì§€ ì¶”ê°€
    if state.get("errors"):
        summary_lines.append("\nâš ï¸ ì²˜ë¦¬ ì¤‘ ë°œìƒí•œ ì´ìŠˆ:")
        for error in state["errors"]:
            summary_lines.append(f"  â€¢ {error}")
    
    return {
        "summary": "\n".join(summary_lines),
        "references": references
    }

# ê¸°ì¡´ ê¸°ë³¸ ì›Œí¬í”Œë¡œìš° (í˜¸í™˜ì„± ìœ ì§€)
basic_graph = StateGraph(ResearchState)

basic_graph.add_node("Keyword Planner", keyword_planner)
basic_graph.add_node("Threads Search", search_threads)
basic_graph.add_node("X Search", search_x)
basic_graph.add_node("Summarize", summarize_results)

basic_graph.set_entry_point("Keyword Planner")
basic_graph.add_edge("Keyword Planner", "Threads Search")
basic_graph.add_edge("Keyword Planner", "X Search")
basic_graph.add_edge("Threads Search", "Summarize")
basic_graph.add_edge("X Search", "Summarize")
basic_graph.add_edge("Summarize", END)

# ê³ ë„í™”ëœ ì›Œí¬í”Œë¡œìš°
enhanced_graph = StateGraph(EnhancedResearchState)

# ë…¸ë“œ ì¶”ê°€
enhanced_graph.add_node("Main Keyword Extractor", extract_main_keyword)
enhanced_graph.add_node("Keyword Breakdown", generate_keyword_breakdown)
enhanced_graph.add_node("Sub Keyword Evaluator", evaluate_sub_keywords)
enhanced_graph.add_node("Advanced Query Generator", generate_advanced_queries)
enhanced_graph.add_node("Enhanced Threads Search", search_threads_enhanced)
enhanced_graph.add_node("Enhanced X Search", search_x_enhanced)
enhanced_graph.add_node("Engagement Analyzer", analyze_engagement_potential)
enhanced_graph.add_node("Strategy Generator", generate_content_strategy)
enhanced_graph.add_node("Enhanced Summarizer", summarize_enhanced_results)

# ì›Œí¬í”Œë¡œìš° ì—°ê²°
enhanced_graph.set_entry_point("Main Keyword Extractor")
enhanced_graph.add_edge("Main Keyword Extractor", "Keyword Breakdown")
enhanced_graph.add_edge("Keyword Breakdown", "Sub Keyword Evaluator")
enhanced_graph.add_edge("Sub Keyword Evaluator", "Advanced Query Generator")
enhanced_graph.add_edge("Advanced Query Generator", "Enhanced Threads Search")
enhanced_graph.add_edge("Advanced Query Generator", "Enhanced X Search")
enhanced_graph.add_edge("Enhanced Threads Search", "Engagement Analyzer")
enhanced_graph.add_edge("Enhanced X Search", "Engagement Analyzer")
enhanced_graph.add_edge("Engagement Analyzer", "Strategy Generator")
enhanced_graph.add_edge("Strategy Generator", "Enhanced Summarizer")
enhanced_graph.add_edge("Enhanced Summarizer", END)

# ì•± ì»´íŒŒì¼
app = basic_graph.compile()  # ê¸°ì¡´ ê¸°ë³¸ ì•± (ê¸°ë³¸ê°’)
enhanced_app = enhanced_graph.compile()  # ê³ ë„í™”ëœ ì•±

# í¸ì˜ë¥¼ ìœ„í•œ ë³„ì¹­
graph = basic_graph  # ê¸°ì¡´ í˜¸í™˜ì„±